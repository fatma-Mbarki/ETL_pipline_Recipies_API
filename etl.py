# -*- coding: utf-8 -*-
"""ETL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RVF9XxDMejRS8cx3zxrIPAs5Khxo0u6R
"""

import requests
import pandas as pd
import logging
import json
import matplotlib.pyplot as plt
import seaborn as sns
import psycopg2
import numpy as np
import nltk
import re
import string
import spacy

import logging

# Create a custom logging filter
class LevelRangeFilter(logging.Filter):
    #check if the log message is within that range
    def  __init__(self, min_level, max_level):
        super().__init__()
        self.min_level = min_level
        self.max_level = max_level

    def filter(self, record):
        # Allow log messages within the specified level range
        return self.min_level <= record.levelno <= self.max_level

# Set up a logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # Log all messages from DEBUG and above

# Create handlers for different log files
success_handler = logging.FileHandler('etl_success.log')
debug_handler = logging.FileHandler('etl_debug.log')
error_handler = logging.FileHandler('etl_errors.log')
console_handler = logging.StreamHandler()  # For console output

# Set a log message format
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
success_handler.setFormatter(formatter)
debug_handler.setFormatter(formatter)
error_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

# Create and add filters to handlers
success_filter = LevelRangeFilter(logging.INFO,logging.WARNING) #INFO MIN / WARNING MAX
error_filter = LevelRangeFilter(logging.WARNING, logging.CRITICAL)
debug_filter = LevelRangeFilter(logging.DEBUG, logging.CRITICAL)

success_handler.addFilter(success_filter)
error_handler.addFilter(error_filter)
debug_handler.addFilter(debug_filter)

# Add handlers to the logger
if not logger.handlers:
    logger.addHandler(success_handler)
    logger.addHandler(debug_handler)
    logger.addHandler(error_handler)
    logger.addHandler(console_handler)

# Start logging
logger.info('ETL pipeline is going to be executed')
logger.debug('This is a debug message')
logger.info('This is an info message')
logger.warning('This is a warning message')
logger.error('This is an error message')
logger.critical('This is a critical message')

def extract(API_URL, API_KEY, number=100):
    logger.debug(f"Starting data extraction from API: {API_URL}")

    try:
        params = {
            'apiKey': API_KEY,
            'number': number
        }
        response = requests.get(API_URL, params=params)

        logger.info(f"Extraction successful, status code: {response.status_code}")

        # Print the raw response text
        print("Raw response text:")
        print(response.text)  # Print raw response to inspect it

        if response.status_code == 200:
            # Check if the response contains valid JSON
            return response.json()
        else:
            logger.error(f"API request failed with status code: {response.status_code}")
            return None
    except Exception as err:
        logger.error("An error occurred", err)
        return None


API_KEY= 'a06c4cec1e1a4a51b6519f4215c8a1b4'
API_URL='https://api.spoonacular.com/recipes/random'
raw_data=extract(API_URL,API_KEY,150)

if raw_data:
    logger.debug(raw_data)
    logger.info("Data extracted successfully.") # Log a success message at the info level
    file_path = "etl.json"

    # Write the JSON data to a file
    with open(file_path, 'w') as json_file:
        json.dump(raw_data, json_file, indent=4)
else:
    # Log an error message if data extraction failed
    logger.error("An error occurred while fetching data.")

import json

def print_json_schema(data, indent=0):
    """
    Recursively prints the schema of a JSON object (dictionary or list).
    """
    if isinstance(data, dict):
        for key, value in data.items():
            print('  ' * indent + f"{key}: {type(value).__name__}")
            if isinstance(value, (dict, list)):
                print_json_schema(value, indent + 1)
    elif isinstance(data, list):
        if len(data) > 0:
            print('  ' * indent + f"List of {type(data[0]).__name__}")
            print_json_schema(data[0], indent + 1)
        else:
            print('  ' * indent + "Empty list")
    else:
        print('  ' * indent + f"Value: {type(data).__name__}")


try:
    # Print the JSON schema
    print("JSON Schema:")
    print_json_schema(raw_data)

    # Optionally print the raw JSON to see the data
    print("\nRaw JSON:")
    print(json.dumps(raw_data, indent=2))  # Pretty-print the JSON data
except Exception as e:
    print(f"Error occurred: {e}")

if raw_data:
    if isinstance(raw_data, dict) and 'recipes' in raw_data:
        df = pd.json_normalize(raw_data['recipes'])

    else:
            logger.error("Unexpected data format")
    print(df.info())
    df.head(5)
    print(df.shape)
else:
    logger.error("No data extracted")

"""# function for cleaning text

---


"""

nlp = spacy.load('en_core_web_sm')

def clean_text_nltk(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize
    tokens = nltk.word_tokenize(text)
    # Remove stopwords
    stop_words = set(nltk.corpus.stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

def clean_text_spacy(text):
    doc = nlp(text)
    # Remove punctuation and stopwords using spaCy
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
    return ' '.join(tokens)

"""### ** recipes dataframe**

---


"""

recipe_column=[ 'veryHealthy',
    'cheap', 'veryPopular', 'sustainable', 'lowFodmap', 'pricePerServing',
    'title', 'readyInMinutes', 'servings', 'sourceUrl', 'summary', 'license','id'
]
#construire data frame recipes
try :
    df_recipes=pd.DataFrame(df, columns=recipe_column)
    logger.info(f"Constructed DataFrame 'recipes' with {len(df_recipes)} recipes.")
    logger.debug(df_recipes.head())
except Exception as e:
    logger.error("Failed to construct DataFrame:",exc_info=True)

try:
    duplicates = df_recipes[df_recipes.duplicated(subset=['title'])]
    logger.debug(duplicates)
    if not duplicates.empty :
        logger.warning(f"Found {len(duplicates)} duplicate recipes. Removing duplicates.")
        df_recipes = df_recipes.drop_duplicates(subset=['title'])
        df_recipes.reset_index(drop=True, inplace=True)
    else:
        logger.info("No duplicates found")
except Exception as e:
    logger.error("Failed to handle duplicates", exc_info=True)



#rename columns
columns_rename= {'pricePerServing':'price_Per_Serving',\
                'title':'recipe_name',\
                'readyInMinutes':'preparation_time',\
                'servings':'servings',\
                'sourceUrl':'source_url',\
                'summary':'description',\
                'id':'recipe_id' }

try:
    df_recipes = df_recipes.rename(columns=columns_rename)
    logger.info("Renamed columns.")
except Exception as e:
    logger.error("Failed to rename columns", exc_info=True)


# Handle missing values: Drop rows with missing values
#df_recipes.dropna(inplace=True)
#drop any  duplicates

df_recipes.drop_duplicates(inplace=True)

# Display the cleaned DataFrame
logger.info(f"Cleaned DataFrame now contains {len(df_recipes)} recipes after removing outliers, duplicates, and handling missing values.")
logger.debug(df_recipes.info())
logger.info(df_recipes.columns)

"""# **ingredients**


---


"""

# Define a function to a list of dictionaries
def extract_dicts(lists):
    """Extract dictionaries from a list of dictionaries and return a list"""
    return [d for d in lists if isinstance(d, dict)]
# Apply the function and sum the results
try:
    # 'all_ingredients' variable recives a list of all dictionaries in the extendedIngredients column
    # note that a dataframe row is a dictionary
    all_extended_ingredients = df['extendedIngredients'].apply(extract_dicts).sum()
    logger.info(f"Extracted {len(all_extended_ingredients)} ingredients.")
except Exception as e:
    logger.error("Failed to extract ingredient dictionaries", exc_info=True)
    all_ingredients = []
try:
  ingredients_df = pd.DataFrame(all_extended_ingredients,columns=['id', 'name', 'aisle', 'image', 'consistency','nameClean'])
  logger.info(f"dataframe ingrediant created with {len('ingrediant_df')} ingrediant")
  logger.info(ingredients_df.info())
except Exception as e:
    logger.error("Failed to construct DataFrame: ",exc_info=True)

#rename les columns

rename_cols={'id':'ingredient_id',\
             'name':'ingredient_name',\
             'image':'image_ingredinet',\
             'nameClean':'clean_name'
            }
try:
  ingredients_df=ingredients_df.rename(columns=rename_cols)
  logger.info('columns renamed')
  logger.info(ingredients_df.head())
  logger.info(ingredients_df.info())
except Exception as e :
  logger.error('Failed to rename columns ', exc_info=True)

#set id as index

try:
    # Set the 'id' column as the index and keep it in the dataframe
    ingredients_df.set_index('ingredient_id',drop=False,inplace=True)
    logger.info("Set 'ingredient_id' column as the index of the DataFrame.")
    logger.debug(ingredients_df.head())
except Exception as e:
    logger.error(f"Failed to set 'id' as the index: {e}", exc_info=True)

# Fill missing 'nameClean' values with 'name' values
try:
    missing_name = ingredients_df[ingredients_df['clean_name'].isna()]
    logger.debug(missing_name)
    if not missing_name.empty:
        logger.warning(f"Found {len(missing_name)} null nameClean. Filling with name.")
        logger.debug(missing_name)
        ingredients_df['nameClean'] = ingredients_df['clean_name'].fillna(ingredients_df['ingredient_name'])
        logger.info("Filled missing 'clean_name' values.")
    else:
        logger.info("No null 'clean_name' found.")
    logger.debug(ingredients_df.info())

    logger.debug(ingredients_df.info())
except Exception as e:
    logger.error(f"Failed to handle null values: {e}", exc_info=True)
# Handle duplicates with same ingreient_id
try :
    duplicates = ingredients_df[ingredients_df.duplicated(subset=['ingredient_id'])]
    logger.debug(duplicates.head(10))
    if not duplicates.empty:
        logger.warning(f"Found {len(duplicates)} duplicate ingredients.")
        ingredients_df = ingredients_df.drop_duplicates(subset=['ingredient_id']).reset_index(drop=True)
        logger.info(f"DataFrame now contains {len(ingredients_df)} ingredients after removing duplicates.")
    else:
        logger.info("No duplicates found.")
    logger.debug(ingredients_df.info())
except Exception as e:
    logger.error(f"Failed to handle duplicate values: {e}", exc_info=True)
#handle duplicate ingreinets using name
try :
    duplicates = ingredients_df[ingredients_df.duplicated(subset=['ingredient_name'])]
    logger.debug(duplicates.head(10))
    if not duplicates.empty:
        logger.warning(f"Found {len(duplicates)} duplicate ingredients.")
        ingredients_df = ingredients_df.drop_duplicates(subset=['ingredient_id']).reset_index(drop=True)
        logger.info(f"DataFrame now contains {len(ingredients_df)} ingredients after removing duplicates.")
    else:
        logger.info("No duplicates found.")
    ingredients_df.head(10)
    logger.info(ingredients_df.columns)
except Exception as e:
    logger.error(f"Failed to handle duplicate values: {e}", exc_info=True)

"""# **recipe_ingredient **"""

recipe_ingredient_data=[]
try :
 for index,recipe in df.iterrows():
     recipes_id = recipe['id']

     if recipes_id in df_recipes['recipe_id'].values :
      try:
        # 'all_ingredients' variable recives a list of all dictionaries in the extendedIngredients column
        # note that a dataframe row is a dictionary
        all_ingredients =df.loc[df['id']==recipes_id ,"extendedIngredients"].apply(extract_dicts).sum()
        logger.info(f"Extracted {len(all_ingredients)} ingredients.")
      except Exception as e:
        logger.error("Failed to extract ingredient dictionaries", exc_info=True)
        all_ingredients = []

      for ingredient in all_ingredients:
          # Extract relevant fields
          ingredient_id = ingredient.get('id')

          # Check if ingredient_id exists in the ingredient_df
          if ingredient_id in ingredients_df['ingredient_id'].values:
              if not any(d['recipe_id'] == recipes_id and d['ingredient_id'] == ingredient_id for d in recipe_ingredient_data):
                  amount = ingredient.get('amount')
                  unit = ingredient.get('unit')
                  # Append to the list
                  recipe_ingredient_data.append({
                      'recipe_id': recipes_id,
                      #'recipe_title':recipe_name,
                      'ingredient_id': ingredient_id,
                      'amount': amount,
                      'unit': unit
                  })
  # Convert the list to a DataFrame
 try :
    recipe_ingredient_df = pd.DataFrame(recipe_ingredient_data)
    logger.debug(f"extracted {len(recipe_ingredient_df)} recipe_ingredient ")

 except Exception as e :
    logger.error("failed to creat data frame recipe_ingredient")
 logger.info(recipe_ingredient_df.head(5))
except Exception as e :
  logger.error("failed to extract data from recipe and ingredient")

print(recipe_ingredient_df.columns)

"""# ***extacting information from instruction  ***

"""

# Load the JSON file
with open("etl.json", "r") as file:
    data = json.load(file)


# Normalize steps from `analyzedInstructions`
def extract_steps(recipe):
    try:
        # Extract analyzedInstructions
        analyzed = recipe.get("analyzedInstructions", [])
        if not analyzed or not isinstance(analyzed, list):
            return []

        # Extract steps with recipe_id
        steps = analyzed[0].get("steps", [])
        for step in steps:
            step["recipe_id"] = recipe["id"]
        return steps
    except Exception as e:
        print(f"Error processing recipe {recipe['id']}: {e}")
        return []
steps = [step for recipe in data["recipes"] for step in extract_steps(recipe)]
steps_df = pd.json_normalize(steps)
logger.info("steps extracted succesfully")
logger.info(steps_df.info())


# Normalize ingredients from steps
def extract_ingredients(recipe):
    try:
        analyzed = recipe.get("analyzedInstructions", [])
        if not analyzed or not isinstance(analyzed, list):
            return []
        ingredients = []
        for step in analyzed[0].get("steps", []):
            for ingredient in step.get("ingredients", []):
                ingredient["recipe_id"] = recipe["id"]
                ingredient["step_number"] = step["number"]
                ingredients.append(ingredient)
        return ingredients
    except Exception as e:
        print(f"Error processing recipe {recipe['id']}: {e}")
        return []

ingredients = [ing for recipe in data["recipes"] for ing in extract_ingredients(recipe)]
ingredients_df = pd.json_normalize(ingredients)
logger.info("ingredient extracted succesfully")
logger.info(ingredients_df.info())


# Normalize equipment from steps
def extract_equipment(recipe):
    try:
        analyzed = recipe.get("analyzedInstructions", [])
        if not analyzed or not isinstance(analyzed, list):
            return []

        equipment = []
        for step in analyzed[0].get("steps", []):
            for equip in step.get("equipment", []):
                equip["recipe_id"] = recipe["id"]
                equip["step_number"] = step["number"]
                equipment.append(equip)
        return equipment
    except Exception as e:
        print(f"Error processing recipe {recipe['id']}: {e}")
        return []

equipment = [equip for recipe in data["recipes"] for equip in extract_equipment(recipe)]
equipment_df = pd.json_normalize(equipment)
logger.info("equipement extracted succesfully")
logger.info(equipment_df.info())

"""# ***cleaning the steps / equipement / ingredient DataFrame***"""

threshold = 0.5  # Drop columns with more than 50% null values

# Function to drop columns with too many null values
def drop_null_columns(df, threshold=0.5):
    null_ratios = df.isnull().mean()
    columns_to_drop = null_ratios[null_ratios > threshold].index
    return df.drop(columns=columns_to_drop)

# Clean each DataFrame by dropping columns with too many null values
try :
  steps_df_cleaned = drop_null_columns(steps_df, threshold)
  logger.info("drpped null data from steps_df succesfully")
  logger.info(steps_df_cleaned.info())
except Exception as e :
  logger.error("failed  to drop columns with too many null values")


try :
  ingredients_df_cleaned = drop_null_columns(ingredients_df, threshold)
  logger.info("drpped null data from ingredients_df succesfully")
  logger.info(ingredients_df_cleaned.info())
except Exception as e :
  logger.error("failed  to drop columns with too many null values")

try :
  equipment_df_cleaned = drop_null_columns(equipment_df, threshold)
  logger.info("drpped null data from equipment_df succesfully")
  logger.info(equipment_df_cleaned.info())
except Exception as e :
  logger.error("failed  to drop columns with too many null values")


# Fill Missing Data
try :
  steps_df_cleaned["step"] = steps_df_cleaned["step"].fillna("")
  ingredients_df_cleaned["name"] = ingredients_df_cleaned["name"].fillna("Unknown ingredient")
  equipment_df_cleaned["name"] = equipment_df_cleaned["name"].fillna("Unknown equipment")
  logger.info("missing values filled succesfully")
except Exception as e :
  logger.error("failed to fill the missing values ")

# combining steps per recipes
try:
    # Combine steps
    combined_steps = steps_df_cleaned.groupby("recipe_id")["step"].apply(lambda x: " ".join(x)).reset_index()
    combined_steps.columns = ["recipe_id", "all_steps"]

    # Include recipes with no steps
    all_recipes_steps = df_recipes[["recipe_id"]].merge(
        combined_steps, on="recipe_id", how="left"
    )
    all_recipes_steps["all_steps"] = all_recipes_steps["all_steps"].fillna("No steps available")

    logger.info("Successfully combined steps with recipes")
    logger.info(all_recipes_steps.head())
except Exception as e:
    logger.error(f"Failed to combine steps, error: {e}")

"""# **Cuisine **"""

dfcuisine=pd.DataFrame(df,columns=["title","cuisine"])
print(dfcuisine.head())